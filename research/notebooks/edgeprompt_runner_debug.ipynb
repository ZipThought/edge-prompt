{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# EdgePrompt Runner Debug Notebook\n",
       "\n",
       "This notebook provides an interactive environment for debugging and exploring the EdgePrompt research framework. It demonstrates how to load results, analyze metrics, and visualize findings."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Setup\n",
       "import os\n",
       "import sys\n",
       "import json\n",
       "import numpy as np\n",
       "import pandas as pd\n",
       "import matplotlib.pyplot as plt\n",
       "import seaborn as sns\n",
       "\n",
       "# Add the parent directory to the Python path\n",
       "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '../..')))\n",
       "\n",
       "# Set up visualization defaults\n",
       "plt.rcParams['figure.figsize'] = (12, 8)\n",
       "sns.set_theme(style=\"whitegrid\")\n",
       "\n",
       "# Import EdgePrompt modules\n",
       "from research.runner.runner_core import RunnerCore\n",
       "from research.runner.config_loader import ConfigLoader\n",
       "from research.runner.model_manager import ModelManager\n",
       "from research.runner.template_engine import TemplateEngine"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Configuration Exploration\n",
       "\n",
       "First, let's explore the available configurations for test suites, hardware profiles, and models."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load hardware profiles\n",
       "hardware_path = '../configs/hardware_profiles.json'\n",
       "with open(hardware_path, 'r') as f:\n",
       "    hardware_profiles = json.load(f)\n",
       "\n",
       "# Display hardware profiles\n",
       "profiles_df = pd.json_normalize(hardware_profiles)\n",
       "profiles_df[['profile_id', 'description', 'simulation_config.max_memory_mb', 'simulation_config.max_cores']]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load model configurations\n",
       "models_path = '../configs/model_configs.json'\n",
       "with open(models_path, 'r') as f:\n",
       "    model_configs = json.load(f)\n",
       "\n",
       "# Display model configurations\n",
       "models_df = pd.json_normalize(model_configs)\n",
       "models_df[['model_id', 'base_model', 'quantization', 'context_window']]"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load a test suite configuration\n",
       "test_suite_path = '../configs/test_suites/multi_stage_validation.json'\n",
       "with open(test_suite_path, 'r') as f:\n",
       "    test_suite = json.load(f)\n",
       "\n",
       "# Display test suite details\n",
       "print(f\"Test Suite: {test_suite['test_suite_id']}\")\n",
       "print(f\"Description: {test_suite['description']}\")\n",
       "print(f\"Templates: {test_suite['templates']}\")\n",
       "print(f\"Models: {test_suite['models']}\")\n",
       "print(f\"Hardware profiles: {test_suite['hardware_profiles']}\")\n",
       "print(f\"Number of test cases: {len(test_suite['test_cases'])}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Interactive Template Processing\n",
       "\n",
       "Now, let's explore how templates are processed with variables."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize the template engine\n",
       "template_engine = TemplateEngine(template_dir='../configs/templates')\n",
       "\n",
       "# Load a template\n",
       "template_name = 'validation_template'\n",
       "template = template_engine.load_template(template_name)\n",
       "\n",
       "# Display template metadata\n",
       "print(f\"Template ID: {template['id']}\")\n",
       "print(f\"Template Type: {template['type']}\")\n",
       "print(f\"\\nConstraints:\")\n",
       "for constraint in template['constraints']:\n",
       "    print(f\"- {constraint}\")\n",
       "\n",
       "# Extract variables\n",
       "variables = template_engine.extract_template_variables(template)\n",
       "print(f\"\\nVariables in template: {variables}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Process a template with example variables\n",
       "example_variables = {\n",
       "    'question': 'What are the main characteristics of rainforests?',\n",
       "    'answer': 'Rainforests have high rainfall, rich biodiversity, warm temperatures, and lush vegetation.'\n",
       "}\n",
       "\n",
       "processed_prompt = template_engine.process_template(template, example_variables)\n",
       "print(\"\\nProcessed Prompt:\")\n",
       "print(processed_prompt)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Simulated Test Execution\n",
       "\n",
       "Let's run a simulated test execution with the framework components."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Initialize the model manager\n",
       "model_manager = ModelManager()\n",
       "\n",
       "# Get model info\n",
       "model_id = 'gemma-3-1b-edge'\n",
       "try:\n",
       "    model_info = model_manager.get_model_info(model_id)\n",
       "    print(f\"Model info for {model_id}:\")\n",
       "    for key, value in model_info.items():\n",
       "        print(f\"- {key}: {value}\")\n",
       "except Exception as e:\n",
       "    print(f\"Error getting model info: {str(e)}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import remaining components\n",
       "from research.runner.test_executor import TestExecutor\n",
       "from research.runner.metrics_collector import MetricsCollector\n",
       "from research.runner.evaluation_engine import EvaluationEngine\n",
       "\n",
       "# Initialize components\n",
       "test_executor = TestExecutor()\n",
       "metrics_collector = MetricsCollector()\n",
       "evaluation_engine = EvaluationEngine()"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Run a simulated test\n",
       "try:\n",
       "    # Initialize model\n",
       "    model = model_manager.initialize_model(model_id)\n",
       "    \n",
       "    # Start metrics collection\n",
       "    metrics_collector.start_collection()\n",
       "    \n",
       "    # Execute test\n",
       "    test_result = test_executor.execute_test(model, processed_prompt)\n",
       "    \n",
       "    # Stop metrics collection\n",
       "    metrics = metrics_collector.stop_collection()\n",
       "    \n",
       "    # Display results\n",
       "    print(f\"Test execution complete in {test_result['execution_time_ms']}ms\")\n",
       "    print(f\"Output: {test_result['output']}\")\n",
       "    print(\"\\nMetrics:\")\n",
       "    for key, value in metrics.items():\n",
       "        if isinstance(value, (int, float)):\n",
       "            print(f\"- {key}: {value:.2f}\")\n",
       "        else:\n",
       "            print(f\"- {key}: {value}\")\n",
       "            \n",
       "except Exception as e:\n",
       "    print(f\"Error running test: {str(e)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Result Analysis\n",
       "\n",
       "If you've already run some experiments, let's analyze the results."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Load results from a JSONL file if it exists\n",
       "results_path = '../data/raw/multi_stage_validation/all_results.jsonl'\n",
       "results = []\n",
       "\n",
       "if os.path.exists(results_path):\n",
       "    with open(results_path, 'r') as f:\n",
       "        for line in f:\n",
       "            try:\n",
       "                results.append(json.loads(line))\n",
       "            except json.JSONDecodeError:\n",
       "                print(f\"Error decoding line in {results_path}\")\n",
       "    \n",
       "    print(f\"Loaded {len(results)} results from {results_path}\")\n",
       "    \n",
       "    # Convert to DataFrame\n",
       "    results_df = pd.json_normalize(results)\n",
       "    \n",
       "    # Display a sample\n",
       "    if not results_df.empty:\n",
       "        print(\"Sample of results:\")\n",
       "        display(results_df[['model_id', 'hardware_profile', 'test_case_id', 'metrics.execution_time_ms', 'validation_result.isValid']].head())\n",
       "    else:\n",
       "        print(\"No results found.\")\n",
       "else:\n",
       "    print(f\"Results file not found: {results_path}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# If results are available, create a visualization\n",
       "if 'results_df' in locals() and not results_df.empty:\n",
       "    try:\n",
       "        # Group by model and hardware profile\n",
       "        performance = results_df.groupby(['model_id', 'hardware_profile']).agg({\n",
       "            'metrics.execution_time_ms': 'mean',\n",
       "            'metrics.memory_usage_mb': 'mean',\n",
       "            'validation_result.isValid': 'mean'\n",
       "        }).reset_index()\n",
       "        \n",
       "        # Rename columns\n",
       "        performance.columns = ['model_id', 'hardware_profile', 'execution_time_ms', 'memory_usage_mb', 'validation_success_rate']\n",
       "        \n",
       "        # Convert validation success rate to percentage\n",
       "        performance['validation_success_rate'] = performance['validation_success_rate'] * 100\n",
       "        \n",
       "        # Create a grouped bar chart for execution time\n",
       "        plt.figure(figsize=(12, 6))\n",
       "        ax = sns.barplot(x='hardware_profile', y='execution_time_ms', hue='model_id', data=performance)\n",
       "        plt.title('Execution Time by Hardware Profile and Model')\n",
       "        plt.xlabel('Hardware Profile')\n",
       "        plt.ylabel('Execution Time (ms)')\n",
       "        plt.xticks(rotation=45)\n",
       "        plt.tight_layout()\n",
       "        plt.show()\n",
       "        \n",
       "        # Create a scatter plot for resource-performance tradeoff\n",
       "        plt.figure(figsize=(10, 6))\n",
       "        ax = sns.scatterplot(x='memory_usage_mb', y='execution_time_ms', hue='model_id', \n",
       "                            size='validation_success_rate', sizes=(50, 250), alpha=0.7, data=performance)\n",
       "        plt.title('Resource-Performance Tradeoff')\n",
       "        plt.xlabel('Memory Usage (MB)')\n",
       "        plt.ylabel('Execution Time (ms)')\n",
       "        \n",
       "        # Add text labels for hardware profiles\n",
       "        for i, row in performance.iterrows():\n",
       "            plt.annotate(row['hardware_profile'], \n",
       "                        (row['memory_usage_mb'], row['execution_time_ms']),\n",
       "                        xytext=(5, 5), textcoords='offset points')\n",
       "            \n",
       "        plt.tight_layout()\n",
       "        plt.show()\n",
       "        \n",
       "    except Exception as e:\n",
       "        print(f\"Error creating visualization: {str(e)}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Manual Runner Execution\n",
       "\n",
       "Finally, we can directly execute the RunnerCore to run a test suite programmatically."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configuration\n",
       "test_suite_path = '../configs/test_suites/multi_stage_validation.json'\n",
       "output_dir = '../data/raw/notebook_test'\n",
       "\n",
       "# Create output directory\n",
       "os.makedirs(output_dir, exist_ok=True)\n",
       "\n",
       "# Initialize the runner (commented out to prevent accidental execution)\n",
       "# runner = RunnerCore(config_path=test_suite_path, output_dir=output_dir, log_level=\"INFO\")\n",
       "\n",
       "# Uncomment to run a test suite\n",
       "# results = runner.run_test_suite()\n",
       "\n",
       "print(\"To run the test suite, uncomment the lines above.\")\n",
       "print(f\"Results will be saved to: {output_dir}\")"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Conclusion\n",
       "\n",
       "This notebook demonstrates how to use the EdgePrompt research framework to process templates, execute tests, and analyze results. You can extend it to perform more advanced analysis and visualization based on your specific research needs."
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }